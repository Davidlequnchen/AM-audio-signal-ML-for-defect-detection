{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e5eeb5-c73a-4e61-89bb-22087d610036",
   "metadata": {},
   "source": [
    "# Acoustic signal processing and feature analysis\n",
    "### Notebook 6 (1): Deep learning feature preparation \n",
    "- Authorï¼š Chen Lequn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5efe3-4c7d-49cf-85ba-53cd9ba812c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Required python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "# plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# plt.rcParams[\"font.serif\"] = \"Times New Roman\"\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8264540-e6ee-4a70-8970-79de2e92542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Audio signal processing libraries\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from scipy.fftpack import fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d3615-150c-48fa-aac0-5d51e0d49cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## others\n",
    "from glob import glob\n",
    "import itertools\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Scikit learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle, resample, class_weight\n",
    "\n",
    "# Keras\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#To visualize the whole grid\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "# matplotlib.rc_file_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa48022e-ba41-45c0-ad4a-843a6211891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- plotly visualizatoin----------------------------------\n",
    "from PIL import Image\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from skimage import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d65c76-0f8f-4a30-9ae1-6bb8a65ad236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scaleogram as scg \n",
    "from glob import glob\n",
    "import glob\n",
    "import re\n",
    "import scipy\n",
    "from scipy.signal import welch\n",
    "import wave                    # library handles the parsing of WAV file headers\n",
    "import pywt\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9dadfe-114d-431e-bb3e-acd4b822b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pytorch libraryes and torchaudio - for GPU accelerated feature extraction\n",
    "# import torch\n",
    "# import torchaudio\n",
    "# from torch import nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import Dataset\n",
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b9c35-ae86-44c4-bdd1-90da181338ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a34bc-d143-4312-af7d-2c5f944d16b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nussl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88452e9f-b9df-421a-97c8-74633c4ed1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures, and dataset locations\n",
    "PROJECT_ROOT_DIR = \"../\"\n",
    "Audio_PATH_original = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\", 'wave_file', \"original\")\n",
    "Audio_PATH_equalized = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\", 'wave_file', \"equalized\")\n",
    "Audio_PATH_bandpassed = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\", 'wave_file', \"bandpassed\")\n",
    "Audio_PATH_denoised = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\",  'wave_file', \"denoised\")\n",
    "\n",
    "Audio_PATH_original_layer = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\",  'wave_file', \"original\", \"sliced_layers\")\n",
    "Audio_PATH_equalized_layer = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\",  'wave_file', \"equalized\", \"sliced_layers\")\n",
    "Audio_PATH_bandpass_layer = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\",  'wave_file', \"bandpassed\", \"sliced_layers\")\n",
    "Audio_PATH_denoised_layer = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\",  'wave_file', \"denoised\", \"sliced_layers\")\n",
    "\n",
    "Audio_PATH_original_segmented = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\",  'wave_file', \"segment\", \"original\") \n",
    "Audio_PATH_equalized_segmented = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\",  'wave_file', \"segment\", \"equalized\")  \n",
    "Audio_PATH_bandpassed_segmented = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\",  'wave_file', \"segment\", \"bandpassed\")\n",
    "Audio_PATH_denoised_segmented = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\",  'wave_file', \"segment\", \"denoised\")\n",
    "\n",
    "\n",
    "label_file = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\",  'wave_file', 'label_v3.csv')\n",
    "\n",
    "Experiment_PATH = os.path.join(PROJECT_ROOT_DIR, \"AM_audio_dataset\")\n",
    "IMAGE_PATH = os.path.join(PROJECT_ROOT_DIR, \"result_images\", \"DL\", \"feature_extraction\")\n",
    "os.makedirs(Audio_PATH_original, exist_ok=True)\n",
    "os.makedirs(IMAGE_PATH, exist_ok=True)\n",
    "\n",
    "## function for automatically save the diagram/graph into the folder \n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGE_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8623604c-7dac-4016-a66b-d378cdd03df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "plt.rcParams[\"axes.linewidth\"] = 2.50\n",
    "plt.rcParams['axes.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae355e-9740-4dd5-8f82-7dca4c1745ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../utils')\n",
    "import glob\n",
    "import utils\n",
    "import filter\n",
    "import feature_extractions\n",
    "from feature_extractions import amplitude_envelope\n",
    "FRAME_SIZE = 512\n",
    "HOP_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3249516-6768-47ac-ad99-156e90371e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d815ab0-631a-4cdc-a858-37da3fff0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import natsort\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3dbe3b-874c-45e7-b16e-98c1c6cb0be7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_wavelet_transform(signal, sr, wavelet = 'shan1.5-1.0', period_length = 200, scale_resolution=5):\n",
    "    # range of scales to perform the transform\n",
    "    # scales = scg.periods2scales(np.arange(1, period_length+1, scale_resolution), wavelet, dt = 1/44100)   # give len of 120\n",
    "    scales=np.arange(1, period_length)\n",
    "    coef, freq = pywt.cwt(signal, scales, wavelet)\n",
    "    return coef, freq\n",
    "\n",
    "def plot_wavelet_transform_resize(signal, sr, wavelet = 'shan1.5-1.0', period_length = 200, scale_resolution=5, rescale_size=120):\n",
    "    # range of scales to perform the transform\n",
    "    # scales = scg.periods2scales(np.arange(1, period_length+1, scale_resolution), wavelet, dt = 1/44100)   # give len of 120\n",
    "    # scales=np.arange(1, period_length)\n",
    "    scales = scg.periods2scales(np.arange(1, period_length+1, scale_resolution))   #len(scales) : 120\n",
    "    # coef, freq = pywt.cwt(signal, scales, wavelet)\n",
    "    coef, freq = scg.fastcwt(signal, scales, wavelet)\n",
    "    \n",
    "    # resize the 2D cwt coeffs \n",
    "    rescale = skimage.transform.resize(abs(coef), (len(scales), rescale_size), mode = 'constant')\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows = 2, ncols = 1, figsize = (8, 8));\n",
    "    \n",
    "    axs[0].imshow(abs(coef), cmap = 'jet', aspect = 'auto')\n",
    "    axs[0].set_title(\"original\")\n",
    "    axs[1].imshow(rescale, cmap = 'jet', aspect = 'auto')\n",
    "    axs[1].set_title(\"resize from 6k to 120 in time axis\")\n",
    "\n",
    "\n",
    "def extract_wavelet_transform_resize(signal, sr, wavelet = 'shan1.5-1.0', period_length = 200, scale_resolution=5, rescale_size=120):\n",
    "    # range of scales to perform the transform\n",
    "    # scales = scg.periods2scales(np.arange(1, period_length+1, scale_resolution), wavelet, dt = 1/44100)   # give len of 120\n",
    "    # scales=np.arange(1, period_length)\n",
    "    scales = scg.periods2scales(np.arange(1, period_length+1, scale_resolution))   #len(scales) : 120\n",
    "    # coef, freq = pywt.cwt(signal, scales, wavelet)\n",
    "    coef, freq = scg.fastcwt(signal, scales, wavelet)\n",
    "    \n",
    "    # resize the 2D cwt coeffs \n",
    "    rescale = skimage.transform.resize(abs(coef), (len(scales), rescale_size), mode = 'constant')\n",
    "    \n",
    "    return rescale\n",
    "\n",
    "\n",
    "def extract_wavelet_transform_fast(signal, sr, wavelet = 'shan1.5-1.0', period_length = 200, scale_resolution=5):\n",
    "    # range of scales to perform the transform\n",
    "    # scales = scg.periods2scales(np.arange(1, period_length+1, scale_resolution), wavelet, dt = 1/44100)   # give len of 120\n",
    "    scales=np.arange(1, period_length)\n",
    "    coef, freq = scg.fastcwt(signal, scales, wavelet)\n",
    "    return coef, freq\n",
    "\n",
    "def plot_wavelet_transform(signal, sr, title, wavelet = 'shan1.5-1.0', period_length = 200, \n",
    "                           scale_resolution=5, set_colorbar = False, cmin=0, cmax=3):\n",
    "    coef, freq = extract_wavelet_transform_fast(signal, sr, wavelet = wavelet, \n",
    "                                                period_length = period_length, scale_resolution=scale_resolution)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    # # fig, axs = plt.subplots(nrows = 2, ncols = 1, figsize = (10, 10));\n",
    "    # fig, (ax1, ax2) = plt.subplots(figsize=(10, 10), nrows=2)\n",
    "    # # plot audio in time domain\n",
    "    # ax1.plot(signal, linewidth = 3, color = 'blue')\n",
    "    # # axs[0].set_title(df_ab['label'][select_list[row]])\n",
    "    # wavelet = ax2.imshow(abs(coef), cmap = 'jet', aspect = 'auto') #aspect = 'auto'\n",
    "    # if (set_colorbar==True):\n",
    "    #     fig.colorbar(wavelet, ax=ax2, location='right', anchor=(0, 0.3), shrink=0.9, (cmin=0, cmax=3))\n",
    "    # else:\n",
    "    #     fig.colorbar(wavelet, ax=ax2, location='right', anchor=(0, 0.3), shrink=0.9)\n",
    "    \n",
    "    wavelet = plt.imshow(abs(coef), cmap = 'jet', aspect = 'auto') #aspect = 'auto'\n",
    "    if (set_colorbar==True):\n",
    "        plt.clim(cmin,cmax)\n",
    "    else:\n",
    "        # plt.clim(-4,4)\n",
    "        pass\n",
    "    \n",
    "        \n",
    "    \n",
    "def plot_scaleogram(signal, sr, title, wavelet = 'shan1.5-1.0', period_length = 600, scale_resolution=50, coi = False, \n",
    "                    yscale = 'linear', set_colorbar = False, cmin=0, cmax=3):\n",
    "    # plot heartbeat in time domain\n",
    "    fig1, ax1 = plt.subplots(1,1, figsize = (9, 3));\n",
    "    ax1.plot(signal, linewidth = 3, color = 'blue')\n",
    "    # ax1.set_xlim(0, signal_length)\n",
    "    ax1.set_title('Time-domain signal')\n",
    "\n",
    "    # choose default wavelet function\n",
    "    scg.set_default_wavelet(wavelet)\n",
    "\n",
    "    # range of scales to perform the transform\n",
    "    # scales = scg.periods2scales(np.arange(1, period_length+1, scale_resolution), wavelet)   # give len of 120\n",
    "    scales=np.arange(1, period_length)\n",
    "    # plot scalogram\n",
    "    if (coi == False and set_colorbar==False):\n",
    "        scg.cws(signal, scales=scales, figsize = (10, 5), coi = coi, ylabel = 'Period',\n",
    "                xlabel = 'Time', title = 'AM acoustic signal Scaleogram ' + title ,\n",
    "                cmap=\"jet\", yscale=yscale\n",
    "                )\n",
    "    elif (set_colorbar==False and coi == True):\n",
    "        scg.cws(signal, scales=scales, figsize = (10, 5), \n",
    "                coi='O',\n",
    "                coikw={'alpha':1.0, 'facecolor':'pink', 'edgecolor':'green',\n",
    "                'linewidth':5} ,\n",
    "                ylabel = 'Period',\n",
    "                xlabel = 'Time', title = 'AM acoustic signal Scaleogram ' + title ,\n",
    "                cmap=\"jet\", yscale=yscale,\n",
    "                )\n",
    "        \n",
    "    elif (set_colorbar==True and coi == False):\n",
    "        scg.cws(signal, scales=scales, figsize = (10, 5), \n",
    "                coi=False,\n",
    "                ylabel = 'Period',\n",
    "                xlabel = 'Time', title = 'AM acoustic signal Scaleogram ' + title ,\n",
    "                cmap=\"jet\", yscale=yscale,\n",
    "                clim=(cmin, cmax)\n",
    "                )\n",
    "    else:\n",
    "        scg.cws(signal, scales=scales, figsize = (10, 5), \n",
    "                coi='O',\n",
    "                coikw={'alpha':1.0, 'facecolor':'pink', 'edgecolor':'green', 'linewidth':5},\n",
    "                ylabel = 'Period',\n",
    "                xlabel = 'Time', title = 'AM acoustic signal Scaleogram ' + title ,\n",
    "                cmap=\"jet\", yscale=yscale,\n",
    "                clim=(cmin, cmax)\n",
    "                )\n",
    "    \n",
    "  \n",
    "    print(\"Wavelet function used to compute the transform:\", scg.get_default_wavelet(), \n",
    "      \"(\", pywt.ContinuousWavelet(scg.get_default_wavelet()).family_name, \")\")\n",
    "    \n",
    "    \n",
    "def plot_scaleogram_resize(signal, sr, title, wavelet = 'shan1.5-1.0', period_length = 600, scale_resolution=50, coi = False, \n",
    "                    yscale = 'linear', set_colorbar = False, cmin=0, cmax=3):\n",
    "    # plot heartbeat in time domain\n",
    "    fig1, ax1 = plt.subplots(1,1, figsize = (9, 3));\n",
    "    ax1.plot(signal, linewidth = 3, color = 'blue')\n",
    "    # ax1.set_xlim(0, signal_length)\n",
    "    ax1.set_title('Time-domain signal')\n",
    "\n",
    "    # choose default wavelet function\n",
    "    scg.set_default_wavelet(wavelet)\n",
    "\n",
    "    # range of scales to perform the transform\n",
    "    # scales = scg.periods2scales(np.arange(1, period_length+1, scale_resolution), wavelet)   # give len of 120\n",
    "    scales=np.arange(1, period_length)\n",
    "    # plot scalogram\n",
    "    if (coi == False and set_colorbar==False):\n",
    "        scg.cws(signal, scales=scales, figsize = (10, 5), coi = coi, ylabel = 'Period',\n",
    "                xlabel = 'Time', title = 'AM acoustic signal Scaleogram ' + title ,\n",
    "                cmap=\"jet\", yscale=yscale\n",
    "                )\n",
    "    elif (set_colorbar==False and coi == True):\n",
    "        scg.cws(signal, scales=scales, figsize = (10, 5), \n",
    "                coi='O',\n",
    "                coikw={'alpha':1.0, 'facecolor':'pink', 'edgecolor':'green',\n",
    "                'linewidth':5} ,\n",
    "                ylabel = 'Period',\n",
    "                xlabel = 'Time', title = 'AM acoustic signal Scaleogram ' + title ,\n",
    "                cmap=\"jet\", yscale=yscale,\n",
    "                )\n",
    "        \n",
    "    elif (set_colorbar==True and coi == False):\n",
    "        scg.cws(signal, scales=scales, figsize = (10, 5), \n",
    "                coi=False,\n",
    "                ylabel = 'Period',\n",
    "                xlabel = 'Time', title = 'AM acoustic signal Scaleogram ' + title ,\n",
    "                cmap=\"jet\", yscale=yscale,\n",
    "                clim=(cmin, cmax)\n",
    "                )\n",
    "    else:\n",
    "        scg.cws(signal, scales=scales, figsize = (10, 5), \n",
    "                coi='O',\n",
    "                coikw={'alpha':1.0, 'facecolor':'pink', 'edgecolor':'green', 'linewidth':5},\n",
    "                ylabel = 'Period',\n",
    "                xlabel = 'Time', title = 'AM acoustic signal Scaleogram ' + title ,\n",
    "                cmap=\"jet\", yscale=yscale,\n",
    "                clim=(cmin, cmax)\n",
    "                )\n",
    "    \n",
    "  \n",
    "    print(\"Wavelet function used to compute the transform:\", scg.get_default_wavelet(), \n",
    "      \"(\", pywt.ContinuousWavelet(scg.get_default_wavelet()).family_name, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e385b-5003-4709-be5a-4c91f231288a",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c062f0-4fa9-4b7e-b7f3-da191f0d1a21",
   "metadata": {},
   "source": [
    "## Load label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905c32b-33a5-46f4-8973-60000c5d998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(label_file)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131c625-2ff4-466a-98f8-0012ae9c2331",
   "metadata": {},
   "source": [
    "## Load audio data (segmented 1-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7f5b8-59aa-4612-a971-b278a0cb0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "AM_wav_original = glob.glob1(Audio_PATH_original_segmented, '*.wav')\n",
    "AM_wav_original = natsort.natsorted(AM_wav_original)\n",
    "for i in range(len(AM_wav_original)):\n",
    "    AM_wav_original[i]= Audio_PATH_original_segmented + \"/\" + AM_wav_original[i]\n",
    "print(f'Total wav file in Audio_PATH_original_segmented folder :{len(AM_wav_original)}')\n",
    "\n",
    "\n",
    "AM_wav_equalized = glob.glob1(Audio_PATH_equalized_segmented, '*.wav')\n",
    "AM_wav_equalized = natsort.natsorted(AM_wav_equalized)\n",
    "for i in range(len(AM_wav_equalized)):\n",
    "    AM_wav_equalized[i]= Audio_PATH_equalized_segmented + \"/\" + AM_wav_equalized[i]\n",
    "print(f'Total wav file in Audio_PATH_equalized_segmented folder :{len(AM_wav_equalized)}')\n",
    "\n",
    "\n",
    "AM_wav_bandpass = glob.glob1(Audio_PATH_bandpassed_segmented, '*.wav')\n",
    "AM_wav_bandpass = natsort.natsorted(AM_wav_bandpass)\n",
    "for i in range(len(AM_wav_bandpass)):\n",
    "    AM_wav_bandpass[i]= Audio_PATH_bandpassed_segmented + \"/\" + AM_wav_bandpass[i]\n",
    "print(f'Total wav file in Audio_PATH_bandpassed_segmented folder :{len(AM_wav_bandpass)}')\n",
    "\n",
    "\n",
    "AM_wav_denoised = glob.glob1(Audio_PATH_denoised_segmented, '*.wav')\n",
    "AM_wav_denoised = natsort.natsorted(AM_wav_denoised)\n",
    "for i in range(len(AM_wav_denoised)):\n",
    "    AM_wav_denoised[i]= Audio_PATH_denoised_segmented + \"/\" + AM_wav_denoised[i]\n",
    "    \n",
    "print(f'Total wav file in Audio_PATH_denoised_segmented folder :{len(AM_wav_denoised)}')\n",
    "\n",
    "\n",
    "dataset_original = []\n",
    "dataset_equalized = []\n",
    "dataset_bandpassed= []\n",
    "dataset_denoised = []\n",
    "\n",
    "\n",
    "def generate_structured_datset(files, dataset_name):\n",
    "    for file in files:\n",
    "        # experiment_22_layer_1_slice_0_raw.wav\n",
    "        experiment_number = int(os.path.basename(file).split(\"_\")[1])  \n",
    "        layer_number = int(os.path.basename(file).split(\"_\")[3])  \n",
    "        slice_number = os.path.basename(file).split(\"_\")[5]\n",
    "        signal_category = os.path.basename(file).split(\"_\")[6].split(\".\")[0]\n",
    "        filename = os.path.basename(file)\n",
    "        filepath = file\n",
    "        dur = librosa.get_duration(filename = file)\n",
    "        signal = librosa.load(file, sr=None)[0]\n",
    "        # compile label wav\n",
    "        dataset_name.append({'filename': filename,\n",
    "                                 'wav_duration' : dur,\n",
    "                                 'file_path' : filepath,\n",
    "                                 'Sample' : experiment_number,\n",
    "                                 'layer' : layer_number,\n",
    "                                 'signal_category' : signal_category,\n",
    "                                 'signal': signal,\n",
    "                                })\n",
    "    print (\"dataset created\")\n",
    "        \n",
    "generate_structured_datset(AM_wav_original, dataset_original)\n",
    "generate_structured_datset(AM_wav_equalized, dataset_equalized)\n",
    "generate_structured_datset(AM_wav_bandpass, dataset_bandpassed)\n",
    "generate_structured_datset(AM_wav_denoised, dataset_denoised)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59663129-1684-430f-9a49-e06a0ca5a8d0",
   "metadata": {},
   "source": [
    "### convert dataset into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a05218-1a1b-4bf5-a94d-3091841e4709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### convert AM_dataset into dataframe\n",
    "df_dataset_original = pd.DataFrame(dataset_original)\n",
    "df_dataset_equalized = pd.DataFrame(dataset_equalized)\n",
    "df_dataset_bandpassed = pd.DataFrame(dataset_bandpassed)\n",
    "df_dataset_denoised = pd.DataFrame(dataset_denoised)\n",
    "\n",
    "print(f'df_dataset_original : {df_dataset_original.shape}')\n",
    "print(f'df_dataset_equalized : {df_dataset_equalized.shape}')\n",
    "print(f'df_dataset_bandpassed : {df_dataset_bandpassed.shape}')\n",
    "print(f'df_dataset_denoised : {df_dataset_denoised.shape}')\n",
    "\n",
    "df_dataset_denoised.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d6e92-911c-4a1e-8c68-29ae06f1268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_dataset_equalized[\"file_path\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d610cf49-1034-4b56-9abe-8fa24680289b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dataset_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e53443-351c-4014-a4d7-6d21530a14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_original = pd.merge(df_dataset_original, labels)\n",
    "df_dataset_equalized = pd.merge(df_dataset_equalized, labels)\n",
    "df_dataset_bandpassed = pd.merge(df_dataset_bandpassed, labels)\n",
    "df_dataset_denoised = pd.merge(df_dataset_denoised, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc594b-1f50-4f50-8a10-2a36c1039be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1610ba9-c8ae-4682-94f3-66f5d604ae67",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50fb5a9-e70e-4cff-b723-df84fdfee51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849984d3-d77f-471b-b99c-6feedbb44a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df_dataset_denoised['Label'].unique() # five unique labels\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d334a2-b8b6-4d5c-939c-58c6e4036225",
   "metadata": {},
   "source": [
    "### Data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc58475-b1c4-4a8b-b4b2-79013c4364f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize = (7,6))\n",
    "\n",
    "\n",
    "ax = sns.countplot(x='Label', data = df_dataset_denoised, palette=\"Set1\"); #palette='mako' 'Set2'\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "# ax = sns.countplot(y='label', data = df_dataset_denoised, palette=\"Set2\");\n",
    "\n",
    "\n",
    "ax.set_title('Distribution of AM audio signal per category', fontsize = 18, pad=12);\n",
    "ax.set_xlabel(\"Experiment number\",fontsize=18, labelpad=10)\n",
    "ax.set_ylabel(\"Count\",fontsize=18, labelpad=10)\n",
    "ax.tick_params(labelsize=15)\n",
    "\n",
    "save_fig(\"dataset_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174e83a-b66e-4517-b69d-c19a3303079d",
   "metadata": {},
   "source": [
    "### Ramdon checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5022c0-46d2-4ea0-99c0-8913e9e371a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal, sr = librosa.load(AM_wav_denoised[20], sr=None)\n",
    "signal = df_dataset_denoised.signal[400]\n",
    "utils.simple_visualization(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b28e4-a417-4a36-bac7-b95be37438e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractions.band_energy_ratio_plot([df_dataset_denoised.signal[400],df_dataset_equalized.signal[400]], frame_size=FRAME_SIZE, \n",
    "                                           hop_length=HOP_LENGTH,sampling_rate = 44100, N_smooth = 1, split_frequency = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389447f8-7dfc-4780-b204-ec5750b75133",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "Train test split the dataset into 80%-20% train-test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917563e-f50c-4583-b306-f74e6c2036e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_denoised.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999bd7fa-1a00-4a2c-a869-5048a5b1cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets:\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0, shuffle=True)\n",
    "train_original, test_original = train_test_split(df_dataset_original, test_size = 0.2, random_state = 123, \n",
    "                                stratify = df_dataset_original.Label, shuffle = True)\n",
    "\n",
    "train_equalized, test_equalized = train_test_split(df_dataset_equalized, test_size = 0.2, random_state = 123, \n",
    "                                stratify = df_dataset_equalized.Label, shuffle = True)\n",
    "\n",
    "train_bandpassed, test_bandpassed = train_test_split(df_dataset_bandpassed, test_size = 0.2, random_state = 123, \n",
    "                                stratify = df_dataset_bandpassed.Label, shuffle = True)\n",
    "\n",
    "train_denoised, test_denoised = train_test_split(df_dataset_denoised, test_size = 0.2, random_state = 123, \n",
    "                                stratify = df_dataset_denoised.Label, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aaebbf-2453-4ab3-b4c3-df27ccc92598",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_equalized.file_path[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6753b-826c-4583-9a1b-6d29fdaac1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_equalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45fd8ee-0504-4f09-9c24-3ebfe2141707",
   "metadata": {},
   "source": [
    "## Feature Extractions from audio wav \n",
    "\n",
    "### MFCC feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aba9425-df19-41c5-8749-e12980306cae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define function to normalize the MFCC\n",
    "\n",
    "Normalization is a crucial preprocessing step. The simplest method is rescaling the range of features to scale the range in `[0, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968fe5e4-115e-41b7-9934-75254c52ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(data):               # data is in numpy array (40,x)\n",
    "    data_normalize = []\n",
    "    data_max = np.max(data, axis =0)       # max value per row, \n",
    "    data_min = np.min(data, axis =0)       # min value per row\n",
    "    for i in range(len(data)):\n",
    "        data_nor = [(x - data_min[i])/(data_max[i] - data_min[i]) for x in data[i]] #normalize each value in 'x'\n",
    "        data_normalize.append(data_nor)\n",
    "    return np.asarray(data_normalize)      #convert back to numpy array\n",
    "\n",
    "### Function for zero padding\n",
    "def zero_pad(data, file_length, pad, truncate):\n",
    "    data = pad_sequences(data, maxlen = file_length, dtype = 'float',\n",
    "                       padding = pad, truncating= truncate, value = 0.)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f721f941-f902-4700-8df7-22305bfa9b7b",
   "metadata": {},
   "source": [
    "### Function to extract MFCC:\n",
    "\n",
    "- MFCC feature extraction\n",
    "- Normalization of MFCC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37000587-7a29-4d40-8f5e-9bdba4d1b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to pre-process and then extract the MFCC feature\n",
    "def extract_feature(data, sr):\n",
    "     \n",
    "    # extract mfcc feature\n",
    "    mfccs = librosa.feature.mfcc(y = data, sr = sr, n_mfcc = 20)\n",
    "    \n",
    "    # mfccs normalization [0,1]\n",
    "    mfccs_sc = feature_normalize(mfccs)\n",
    "    \n",
    "    return mfccs_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d16c79-42b8-47e9-a59b-7e1a2798052f",
   "metadata": {},
   "source": [
    "### Extract X-features\n",
    "\n",
    "- These includes **train, test** datasets and also the unlabel dataset from set_a & set_b\n",
    "- The data is with imbalance class treatment (synthesize new sample + oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570b4be-869f-4cbb-81cc-4abe8dfddb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "### load labeled dataset (i.e.: both train & test)\n",
    "### resuls of dataset are compilation of data with the same length of samples \n",
    "sfreq = 44100\n",
    "\n",
    "dataset_train_original = []\n",
    "dataset_test_original = []\n",
    "\n",
    "dataset_train_equalized = []\n",
    "dataset_test_equalized = []\n",
    "\n",
    "dataset_train_bandpassed = []\n",
    "dataset_test_bandpassed = []\n",
    "\n",
    "dataset_train_denoised = []\n",
    "dataset_test_denoised = []\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "dataset_train_original = train_original.signal.to_list()\n",
    "dataset_test_original = test_original.signal.to_list()\n",
    "\n",
    "dataset_train_equalized = train_equalized.signal.to_list()\n",
    "dataset_test_equalized = test_equalized.signal.to_list()\n",
    "\n",
    "dataset_train_bandpassed = train_bandpassed.signal.to_list()\n",
    "dataset_test_bandpassed = test_bandpassed.signal.to_list()\n",
    "\n",
    "dataset_train_denoised = train_denoised.signal.to_list()\n",
    "dataset_test_denoised = test_denoised.signal.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321fcba-48f2-4e44-9902-9bde9ab5d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_original.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c8f94-f9c6-452c-b65c-71023ab97466",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "### Extract mfcc feature\n",
    "X_train_original = []\n",
    "X_test_original = []\n",
    "\n",
    "for i in range(len(dataset_train_original)):\n",
    "    X_train_original.append(extract_feature(dataset_train_original[i], sfreq))\n",
    "\n",
    "for i in range(len(dataset_test_original)):\n",
    "    X_test_original.append(extract_feature(dataset_test_original[i], sfreq))\n",
    "    \n",
    "### change X to array\n",
    "X_train_original = np.asarray(X_train_original)\n",
    "X_test_original = np.asarray(X_test_original)\n",
    "\n",
    "print(f'X_train_original: {X_train_original.shape}')\n",
    "print(f'X_test_original: {X_test_original.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d97b47e-bcee-450c-9ed1-f38e47bcf510",
   "metadata": {},
   "source": [
    "### get the target: y_train & y_test \n",
    "\n",
    "This can be share with other set of feature (e.g., CWT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7521d-f6c3-4d62-80b0-5f063bc5d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract label as y-target and one-hot encode to integer\n",
    "y_train_original = train_original.Label\n",
    "y_test_original = test_original.Label\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_original = le.fit_transform(y_train_original)\n",
    "y_test_original = le.transform(y_test_original)\n",
    "\n",
    "# to get back the label, use le.inverse_transform(ys)\n",
    "print(f\"y_train_original: {y_train_original.shape}\")\n",
    "print(f\"y_test_original: {y_test_original.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a50ceb7-85c7-4565-a2c9-5075c04c42cb",
   "metadata": {},
   "source": [
    "### Compute class weight\n",
    "\n",
    "This is the 3rd step of treating the imbalanced class, which is calculating the class weight and use it for weighting the loss function (during training only). This steps need to repeat for 2nd set of feature (CWT)\n",
    "\n",
    "`'balanced'` => n_samples / (n_classes * np.bincount(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5271d-a532-4bc5-b03c-ff74015b02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_original = class_weight.compute_class_weight('balanced',\n",
    "                                                 classes = np.unique(y_train_original),\n",
    "                                                 y = y_train_original)\n",
    "print(f\"class weights: {class_weights_original}\")\n",
    "print(f\"class        : {np.unique(le.inverse_transform(y_train_original))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb76310-78ae-4c27-9a13-601d8e97488d",
   "metadata": {},
   "source": [
    "### Re-shape X and change y to dummies (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b74cc85-5a37-4bce-bcce-9c9bf7db2eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original = X_train_original.reshape(X_train_original.shape[0], X_train_original.shape[1], X_train_original.shape[2], 1)\n",
    "X_test_original = X_test_original.reshape(X_test_original.shape[0], X_test_original.shape[1], X_test_original.shape[2], 1)\n",
    "\n",
    "### change int categorical to dummies\n",
    "y_train_original = to_categorical(y_train_original)\n",
    "y_test_original = to_categorical(y_test_original)\n",
    "\n",
    "print(f'shape X_train_original : {X_train_original.shape}')\n",
    "print(f'shape X_test_original : {X_test_original.shape}')\n",
    "print(f'shape y_train_original : {y_train_original.shape}')\n",
    "print(f'shape y_test_original : {y_test_original.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55ee6e-2ca5-4fd5-b816-74ee0b1b8023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f653a3-9eae-4893-a2c2-efb067ec3eb5",
   "metadata": {},
   "source": [
    "### Store the pre-processed data for modelling in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ef595-21e3-4a5f-8104-7cbca4fcc487",
   "metadata": {},
   "outputs": [],
   "source": [
    "### store the preprocessed data for use in the next notebook\n",
    "\n",
    "%store X_train_original \n",
    "%store X_test_original \n",
    "%store y_train_original\n",
    "%store y_test_original \n",
    "%store le\n",
    "%store class_weights_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae387fd-f7fc-4f44-9c4a-e5802e027a51",
   "metadata": {},
   "source": [
    "# Wrap the entire process into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ecef78-cde1-43d5-b9f2-cb9eef13423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_feature_dataset_prepare(dataset_train, dataset_test, train, test, sfreq=44100):\n",
    "    %time\n",
    "    \n",
    "    \n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    \n",
    "    for i in range(len(dataset_train)):\n",
    "        X_train.append(extract_feature(dataset_train[i], sfreq))\n",
    "\n",
    "    for i in range(len(dataset_test)):\n",
    "        X_test.append(extract_feature(dataset_test[i], sfreq))\n",
    "\n",
    "    ### change X to array\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_test = np.asarray(X_test)\n",
    "\n",
    "    print(f'X_train: {X_train.shape}')\n",
    "    print(f'X_test: {X_test.shape}')\n",
    "    \n",
    "    ### Extract label as y-target and one-hot encode to integer\n",
    "    y_train = train.Label\n",
    "    y_test = test.Label\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "    # to get back the label, use le.inverse_transform(ys)\n",
    "    print(f\"y_train: {y_train.shape}\")\n",
    "    print(f\"y_test: {y_test.shape}\")\n",
    "    \n",
    "    class_weights= class_weight.compute_class_weight('balanced',\n",
    "                                                     classes = np.unique(y_train),\n",
    "                                                     y = y_train)\n",
    "    print(f\"class weights: {class_weights}\")\n",
    "    print(f\"class        : {np.unique(le.inverse_transform(y_train))}\")\n",
    "    \n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "\n",
    "    ### change int categorical to dummies\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    print(f'shape X_train : {X_train.shape}')\n",
    "    print(f'shape X_test : {X_test.shape}')\n",
    "    print(f'shape y_train : {y_train.shape}')\n",
    "    print(f'shape y_test : {y_test.shape}')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, le, class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401e9c5-5f96-4bee-bafc-0d2658d3c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_equalized, X_test_equalized, y_train_equalized, y_test_equalized, le_equalized, class_weights_equalized = extract_mfcc_feature_dataset_prepare(dataset_train_equalized,\n",
    "dataset_test_equalized, train_equalized, test_equalized)\n",
    "\n",
    "X_train_bandpassed, X_test_bandpassed, y_train_bandpassed, y_test_bandpassed, le_bandpassed, class_weights_bandpassed = extract_mfcc_feature_dataset_prepare(dataset_train_bandpassed,\n",
    "dataset_test_bandpassed, train_bandpassed, test_bandpassed)\n",
    "\n",
    "X_train_denoised, X_test_denoised, y_train_denoised, y_test_denoised, le_denoised, class_weights_denoised = extract_mfcc_feature_dataset_prepare(dataset_train_denoised,\n",
    "dataset_test_denoised, train_denoised, test_denoised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba82cb-cc69-448c-a036-73ce38a373f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### store the preprocessed data for use in the next notebook\n",
    "\n",
    "%store X_train_equalized \n",
    "%store X_test_equalized \n",
    "%store y_train_equalized\n",
    "%store y_test_equalized \n",
    "%store le_equalized\n",
    "%store class_weights_equalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f23621-740c-4c5f-b7a3-816ddad1cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "### store the preprocessed data for use in the next notebook\n",
    "\n",
    "%store X_train_bandpassed \n",
    "%store X_test_bandpassed \n",
    "%store y_train_bandpassed\n",
    "%store y_test_bandpassed \n",
    "%store le_bandpassed\n",
    "%store class_weights_bandpassed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf800e-e507-46cd-a279-ca7941fcec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### store the preprocessed data for use in the next notebook\n",
    "\n",
    "%store X_train_denoised \n",
    "%store X_test_denoised\n",
    "%store y_train_denoised\n",
    "%store y_test_denoised \n",
    "%store le_denoised\n",
    "%store class_weights_denoised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06194a87-13b7-4c22-95d1-f5ce9b4925fe",
   "metadata": {},
   "source": [
    "#### Proceed to notebook 7 for DL modelling "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
